{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =sc.textFile(\"/user/ncn251/sunspots.txt\").map(lambda line: np.array([float(x) for x in line.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3143"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different values of k are checked before arriving to the conclusion of using k as 10. \n",
    "#value of k can be find by cheking the output error for each value of k and based on that the best value of k can be found out\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "clusters = KMeans.train(df, 10, maxIterations=100,runs=1, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes = df.map(lambda e: clusters.predict(e)).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 297,\n",
       "             4: 294,\n",
       "             7: 317,\n",
       "             3: 306,\n",
       "             1: 306,\n",
       "             6: 318,\n",
       "             9: 316,\n",
       "             2: 328,\n",
       "             8: 332,\n",
       "             5: 329})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(clusters):\n",
    "    def get_distance_map(record):\n",
    "        cluster = clusters.predict(record)\n",
    "        centroid = clusters.centers[cluster]\n",
    "        dist = np.linalg.norm(record - centroid)\n",
    "        return (float(dist), record.tolist(), cluster)\n",
    "    return get_distance_map\n",
    "\n",
    "def createDF(data):\n",
    "    return Row(distance=data[0], element=data[1], cluster=data[2])\n",
    "\n",
    "data_distance = df.map(get_distance(clusters))\n",
    "dataFrame=spark.createDataFrame(data_distance.map(lambda x: createDF(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created clusters using k means algorithm\n",
    "#All the points which are 2 standard deviations away from the centroid of the cluster are considered outliers \n",
    "dataFrame.createOrReplaceTempView(\"anomaly_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgPerCluster=spark.sql(\"select avg(distance) as average,cluster from anomaly_detection group by cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgPerCluster.createOrReplaceTempView(\"avg_per_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanPerData=spark.sql(\"select ad.*, avc.average, pow(average-distance,2) as sum_square from anomaly_detection as ad inner join avg_per_cluster as avc on ad.cluster=avc.cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanPerData.createOrReplaceTempView(\"sum_square_per_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_per_cluster=spark.sql(\"select cluster, avg(sum_square) as variance from sum_square_per_cluster group by cluster\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_per_cluster.createOrReplaceTempView(\"variance_per_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_per_cluster=spark.sql(\"select cluster, sqrt(variance) as std from variance_per_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_per_cluster.createOrReplaceTempView(\"std_per_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers=spark.sql(\"select element from sum_square_per_cluster as sspc inner join std_per_cluster as std on sspc.cluster=std.cluster where distance> average+2*std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|        element|\n",
      "+---------------+\n",
      "|  [10.0, 158.6]|\n",
      "|[1185.0, 180.4]|\n",
      "|[1199.0, 159.5]|\n",
      "|[1200.0, 157.0]|\n",
      "|[2504.0, 235.8]|\n",
      "|[2505.0, 253.8]|\n",
      "|[2507.0, 239.4]|\n",
      "| [352.0, 238.9]|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-2.3.0 / PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
